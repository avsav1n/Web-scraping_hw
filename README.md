# [Программа Web-scraping'а сайтов поиска работы](https://github.com/netology-code/py-homeworks-advanced/tree/new_hw_scrapping/6.Web-scrapping)
## О программе

Данная программа осуществляет Web-скрапинг сайтов поиска работы (реализован парсинг таких сайтов как career.habr.com и hh.ru) по заданным параметрам: основной тэг, города (доступны только Москва и Санкт-Петербург), дополнительные тэги и количество страниц для обработки (одна страница на career.habr.com соответствует 25-ти вакансиям, на hh.ru - 20-ти). 

Программа полностью автоматизирована - после запуска программы, скрипт по расписанию (ежедневно в 16:00 для career.habr.com и 17:00 для hh.ru) будет самостоятельно парсить данные сайты на наличие удовлетворяющих параметрам вакансий.

Результатом скрапинга являются файлы формата .json (`habrcareer_vacancies.json` для career.habr.com и `headhunter_vacancies.json` для hh.ru), содержащие словарь о подходящих вакансиях с основными параметрами (ссылка, должность, зарплата, компания, адрес): 
```python
    {
    'new': [{'link': str, 'position': str, 'salary': str, 'company': str, 'address': str}, {...}],
    'old': [{'link': str, 'position': str, 'salary': str, 'company': str, 'address': str}, {...}]
    }
```
После каждого активации скрипта устаревшие данные (вчерашние) в процессе работы программы перемещаются под ключ `'old'`, а новые вакансии, отсутствовавшие в словаре ранее, попадают под ключ `'new'`.

## Структура программы, модули и библиотеки

Программа состоит из двух модулей:

1. Основной модуль [**`main.py`**](main.py)

Cодержит основную логику работы скрипта. Состоит из двух основных статических классов `HeadHunter` и `HabrCareer`, каждый из которых отвечает за взаимодействия со своим сайтом.
Также данные классы отличаются реализацией инструментов скрапинга - `HeadHunter` осуществляет парсинг с помощью библиотек `requests` и `beautifulsoup4`, `HabrCareer` - `selenium`.

2. Модуль [**`searching_config.py`**](searching_config.py)

Для удобства настройки поиска основные константы вынесены в данный модуль:

```python
# целевой запрос
MAIN_TAG = 'python'
# интересуемые регионы
CITIES = ['Москва', 'Санкт-Петербург']  
# дополнительные параметры
EXTRA_TAGS = ['SQL']
# количество страниц
NUMBER_OF_PAGES = 3
```
3. Сторонние библиотеки

В качестве основных сторонних библиотек, необходимых для работы программы используются [requests](https://pypi.org/project/requests/), [beautifulsopu4](https://pypi.org/project/beautifulsoup4/) и [selenium](https://pypi.org/project/selenium/). Используемые при написании и тестрировании программы библиотеки указаны в [requirements.txt](requirements.txt)

## Подготовка, запуск и работа

Перед запуском программы необходимо задать параметры интересующих вакансий в модуле [**`searching_config.py`**](searching_config.py), после чего осуществить запуск скрипта из основного модуля [**`main.py`**](main.py).

Для удобства отслеживания работы программы в основной модуль встроено логгирование ключевых этапов выполнения скрипта. Логи доступны в файле `progress.log`